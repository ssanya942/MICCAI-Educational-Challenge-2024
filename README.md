# Implementing Vision Transformers in PyTorch from Scratch on Any Dataset!
![Intro Image (1)](https://github.com/user-attachments/assets/437c8d37-9ae6-49a6-aec2-887c364349c5)

## Introduction-Riding the Classics
Vision Transformers (ViTs) were introduced in 2020 to present a classification approach for images. Having being inspired by the Transformer architecture used prevalently in text classification, ViTs present a robust approach powered by multi-head self-attention to compete with CNNs for image classification/segmentation tasks. There have been numerous implementations of ViTs so far that capture the performance of the model perfectly. However, they are often implementable through external APIs such as [HuggingFace](https://huggingface.co/docs/transformers/en/model_doc/vit) or [Keras](https://keras.io/examples/vision/image_classification_with_vision_transformer/), providing easier computations at the expense of lower architectural transparency. In fact, while these APIs are a Godsend for developers and engineers to facilitate model implementation and fine-tuning at the hit of a button, they are often not the best resort for students starting with Deep Learning. For learners, the best approach to get hands-on deep learning practice is through implementing SOTA models from scratch. In this repository, we have created the most lucid and understandable tutorial for students to implement ViTs from scratch on any dataset of their choice. 

## Installation
Please clone the repository to install all the required dependencies and navigate to the working directory. 
```bash
!git clone https://github.com/ssanya942/MICCAI-Educational-Challenge-2024.git
%cd /content/MICCAI-Educational-Challenge-2024
```




